[{"citedPaper": {"paperId": "8e19265e4e20575c292c89a2d9faa85780498709", "url": "https://www.semanticscholar.org/paper/8e19265e4e20575c292c89a2d9faa85780498709", "title": "A Unified Deep Learning Anomaly Detection and Classification Approach for Smart Grid Environments", "abstract": "The interconnected and heterogeneous nature of the next-generation Electrical Grid (EG), widely known as Smart Grid (SG), bring severe cybersecurity and privacy risks that can also raise domino effects against other Critical Infrastructures (CIs). In this paper, we present an Intrusion Detection System (IDS) specially designed for the SG environments that use Modbus/Transmission Control Protocol (TCP) and Distributed Network Protocol 3 (DNP3) protocols. The proposed IDS called MENSA (anoMaly dEtection aNd claSsificAtion) adopts a novel Autoencoder-Generative Adversarial Network (GAN) architecture for (a) detecting operational anomalies and (b) classifying Modbus/TCP and DNP3 cyberattacks. In particular, MENSA combines the aforementioned Deep Neural Networks (DNNs) in a common architecture, taking into account the adversarial loss and the reconstruction difference. The proposed IDS is validated in four real SG evaluation environments, namely (a) SG lab, (b) substation, (c) hydropower plant and (d) power plant, solving successfully an outlier detection (i.e., anomaly detection) problem as well as a challenging multiclass classification problem consisting of 14 classes (13 Modbus/TCP cyberattacks and normal instances). Furthermore, MENSA can discriminate five cyberattacks against DNP3. The evaluation results demonstrate the efficiency of MENSA compared to other Machine Learning (ML) and Deep Learning (DL) methods in terms of Accuracy, False Positive Rate (FPR), True Positive Rate (TPR) and the F1 score.", "year": 2021}}, {"citedPaper": {"paperId": "8c39251358d0c60f28a5422c78554766d3685f02", "url": "https://www.semanticscholar.org/paper/8c39251358d0c60f28a5422c78554766d3685f02", "title": "Explainable decision forest: Transforming a decision forest into an interpretable tree", "abstract": null, "year": 2020}}, {"citedPaper": {"paperId": "b5e2e223fb5f85e97a7df1e6e4a01540b6953fb7", "url": "https://www.semanticscholar.org/paper/b5e2e223fb5f85e97a7df1e6e4a01540b6953fb7", "title": "Explainable Artificial Intelligence for 6G: Improving Trust between Human and Machine", "abstract": "As 5G mobile networks are bringing about global societal benefits, the design phase for 6G has started. Evolved 5G and 6G will need sophisticated AI to automate information delivery simultaneously for mass autonomy, human machine interfacing, and targeted healthcare. Trust will become increasingly critical for 6G as it manages a wide range of mission-critical services. As we migrate from traditional mathematical model-dependent optimization to data-dependent deep learning, the insight and trust we have in our optimization modules decrease. This loss of model explainability means we are vulnerable to malicious data, poor neural network design, and the loss of trust from stakeholders and the general public -- all with a range of legal implications. In this review, we outline the core methods of explainable artificial intelligence (XAI) in a wireless network setting, including public and legal motivations, definitions of explainability, performance vs. explainability trade-offs, and XAI algorithms. Our review is grounded in case studies for both wireless PHY and MAC layer optimization and provide the community with an important research area to embark upon.", "year": 2020}}, {"citedPaper": {"paperId": "4470e0462be50fbe02dbb0d643091b0ff741c363", "url": "https://www.semanticscholar.org/paper/4470e0462be50fbe02dbb0d643091b0ff741c363", "title": "An Explainable Intelligence Model for Security Event Analysis", "abstract": null, "year": 2019}}, {"citedPaper": {"paperId": "530a059cb48477ad1e3d4f8f4b153274c8997332", "url": "https://www.semanticscholar.org/paper/530a059cb48477ad1e3d4f8f4b153274c8997332", "title": "Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI", "abstract": null, "year": 2019}}, {"citedPaper": {"paperId": "840ad2146a494206af610c211be7ace603396d0e", "url": "https://www.semanticscholar.org/paper/840ad2146a494206af610c211be7ace603396d0e", "title": "Enriching Visual with Verbal Explanations for Relational Concepts - Combining LIME with Aleph", "abstract": null, "year": 2019}}, {"citedPaper": {"paperId": "debbf0634c04d67e4df9db600150d574f3b16980", "url": "https://www.semanticscholar.org/paper/debbf0634c04d67e4df9db600150d574f3b16980", "title": "Human-Centric AI for Trustworthy IoT Systems With Explainable Multilayer Perceptrons", "abstract": "Internet of Things (IoT) widely use analysis of data with artificial intelligence (AI) techniques in order to learn from user actions, support decisions, track relevant aspects of the user, and notify certain events when appropriate. However, most AI techniques are based on mathematical models that are difficult to understand by the general public, so most people use AI-based technology as a black box that they eventually start to trust based on their personal experience. This article proposes to go a step forward in the use of AI in IoT, and proposes a novel approach within the Human-centric AI field for generating explanations about the knowledge learned by a neural network (in particular a multilayer perceptron) from IoT environments. More concretely, this work proposes two techniques based on the analysis of artificial neuron weights, and another technique aimed at explaining each estimation based on the analysis of training cases. This approach has been illustrated in the context of a smart IoT kitchen that detects the user depression based on the food used for each meal, using a simulator for this purpose. The results revealed that most auto-generated explanations made sense in this context (i.e. 97.0%), and the execution times were low (i.e. 1.5 ms or lower) even considering the common configurations varying independently the number of neurons per hidden layer (up to 20), the number of hidden layers (up to 20) and the number of training cases (up to 4,000).", "year": 2019}}, {"citedPaper": {"paperId": "c86e78458721a0d02ece7ca376e60cb0ed076377", "url": "https://www.semanticscholar.org/paper/c86e78458721a0d02ece7ca376e60cb0ed076377", "title": "Explainable AI in Industry", "abstract": "Artificial Intelligence is increasingly playing an integral role in determining our day-to-day experiences. Moreover, with proliferation of AI based solutions in areas such as hiring, lending, criminal justice, healthcare, and education, the resulting personal and professional implications of AI are far-reaching. The dominant role played by AI models in these domains has led to a growing concern regarding potential bias in these models, and a demand for model transparency and interpretability. In addition, model explainability is a prerequisite for building trust and adoption of AI systems in high stakes domains requiring reliability and safety such as healthcare and automated transportation, and critical industrial applications with significant economic implications such as predictive maintenance, exploration of natural resources, and climate change modeling. As a consequence, AI researchers and practitioners have focused their attention on explainable AI to help them better trust and understand models at scale. The challenges for the research community include (i) defining model explainability, (ii) formulating explainability tasks for understanding model behavior and developing solutions for these tasks, and finally (iii) designing measures for evaluating the performance of models in explainability tasks. In this tutorial, we will present an overview of model interpretability and explainability in AI, key regulations/laws, and techniques/tools for providing explainability as part of AI/ML systems. Then, we will focus on the application of explainability techniques in industry, wherein we present practical challenges/ guidelines for using explainability techniques effectively and lessons learned from deploying explainable models for several web-scale machine learning and data mining applications. We will present case studies across different companies, spanning application domains such as search and recommendation systems, sales, lending, and fraud detection. Finally, based on our experiences in industry, we will identify open problems and research directions for the data mining/machine learning community.", "year": 2019}}, {"citedPaper": {"paperId": "38f23fe236b152cd4983c8f30d305a568afd0d3e", "url": "https://www.semanticscholar.org/paper/38f23fe236b152cd4983c8f30d305a568afd0d3e", "title": "A Survey on Explainable Artificial Intelligence (XAI): Toward Medical XAI", "abstract": "Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning (DL). Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example, the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the DL is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide \u201cobviously\u201d interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that: 1) clinicians and practitioners can subsequently approach these methods with caution; 2) insight into interpretability will be born with more considerations for medical practices; and 3) initiatives to push forward data-based, mathematically grounded, and technically grounded medical education are encouraged.", "year": 2019}}, {"citedPaper": {"paperId": "72be39943d06037a11c804c36fce652494f6404c", "url": "https://www.semanticscholar.org/paper/72be39943d06037a11c804c36fce652494f6404c", "title": "Explainable Artificial Intelligence Applications in NLP, Biomedical, and Malware Classification: A Literature Review", "abstract": null, "year": 2019}}, {"citedPaper": {"paperId": "c72a7a60334794f78d612fb67669445e24755b21", "url": "https://www.semanticscholar.org/paper/c72a7a60334794f78d612fb67669445e24755b21", "title": "AI in Healthcare: Ethical and Privacy Challenges", "abstract": null, "year": 2019}}, {"citedPaper": {"paperId": "e5c703aba8af983c36fedf08c32a6978eadd91b9", "url": "https://www.semanticscholar.org/paper/e5c703aba8af983c36fedf08c32a6978eadd91b9", "title": "DLIME: A Deterministic Local Interpretable Model-Agnostic Explanations Approach for Computer-Aided Diagnosis Systems", "abstract": "Local Interpretable Model-Agnostic Explanations (LIME) is a popular technique used to increase the interpretability and explainability of black box Machine Learning (ML) algorithms. LIME typically generates an explanation for a single prediction by any ML model by learning a simpler interpretable model (e.g. linear classifier) around the prediction through generating simulated data around the instance by random perturbation, and obtaining feature importance through applying some form of feature selection. While LIME and similar local algorithms have gained popularity due to their simplicity, the random perturbation and feature selection methods result in \"instability\" in the generated explanations, where for the same prediction, different explanations can be generated. This is a critical issue that can prevent deployment of LIME in a Computer-Aided Diagnosis (CAD) system, where stability is of utmost importance to earn the trust of medical professionals. In this paper, we propose a deterministic version of LIME. Instead of random perturbation, we utilize agglomerative Hierarchical Clustering (HC) to group the training data together and K-Nearest Neighbour (KNN) to select the relevant cluster of the new instance that is being explained. After finding the relevant cluster, a linear model is trained over the selected cluster to generate the explanations. Experimental results on three different medical datasets show the superiority for Deterministic Local Interpretable Model-Agnostic Explanations (DLIME), where we quantitatively determine the stability of DLIME compared to LIME utilizing the Jaccard similarity among multiple generated explanations.", "year": 2019}}, {"citedPaper": {"paperId": "274a0c1b9bbf1f7e3f4ba3fc63b1061c2f709b85", "url": "https://www.semanticscholar.org/paper/274a0c1b9bbf1f7e3f4ba3fc63b1061c2f709b85", "title": "Machine Learning-Based Network Vulnerability Analysis of Industrial Internet of Things", "abstract": "It is critical to secure the Industrial Internet of Things (IIoT) devices because of potentially devastating consequences in case of an attack. Machine learning (ML) and big data analytics are the two powerful leverages for analyzing and securing the Internet of Things (IoT) technology. By extension, these techniques can help improve the security of the IIoT systems as well. In this paper, we first present common IIoT protocols and their associated vulnerabilities. Then, we run a cyber-vulnerability assessment and discuss the utilization of ML in countering these susceptibilities. Following that, a literature review of the available intrusion detection solutions using ML models is presented. Finally, we discuss our case study, which includes details of a real-world testbed that we have built to conduct cyber-attacks and to design an intrusion detection system (IDS). We deploy backdoor, command injection, and Structured Query Language (SQL) injection attacks against the system and demonstrate how a ML-based anomaly detection system can perform well in detecting these attacks. We have evaluated the performance through representative metrics to have a fair point of view on the effectiveness of the methods.", "year": 2019}}, {"citedPaper": {"paperId": "aaab17eb7069b3f80b249331a27d7e11f0d85fbe", "url": "https://www.semanticscholar.org/paper/aaab17eb7069b3f80b249331a27d7e11f0d85fbe", "title": "Single Image Dehazing with a Generic Model-Agnostic Convolutional Neural Network", "abstract": "A simple convolutional neural network is proposed in this letter and is trained end-to-end to restore clear images from hazy inputs. The proposed network is generic and agnostic in the sense that it is not designed specifically for image dehazing and, in particular, it has no knowledge of the atmosphere scattering model. Remarkably, this network achieves record-breaking dehazing performance on several standard data sets that are synthesized using the atmosphere scattering model. This surprising finding suggests that there might be a need to rethink the predominant plug-in approach to image dehazing.", "year": 2019}}, {"citedPaper": {"paperId": "8ec4b08a83faf5622fea30888a7fe14b7b0baf9b", "url": "https://www.semanticscholar.org/paper/8ec4b08a83faf5622fea30888a7fe14b7b0baf9b", "title": "Internet of Things: A survey on machine learning-based intrusion detection approaches", "abstract": null, "year": 2019}}, {"citedPaper": {"paperId": "ca62c9bfa35fd5496b357b6f9c6f0eecdae1062a", "url": "https://www.semanticscholar.org/paper/ca62c9bfa35fd5496b357b6f9c6f0eecdae1062a", "title": "Effect of Imbalanced Datasets on Security of Industrial IoT Using Machine Learning", "abstract": "Machine learning algorithms have been shown to be suitable for securing platforms for IT systems. However, due to the fundamental differences between the industrial internet of things (IIoT) and regular IT networks, a special performance review needs to be considered. The vulnerabilities and security requirements of IIoT systems demand different considerations. In this paper, we study the reasons why machine learning must be integrated into the security mechanisms of the IIoT, and where it currently falls short in having a satisfactory performance. The challenges and real-world considerations associated with this matter are studied in our experimental design. We use an IIoT testbed resembling a real industrial plant to show our proof of concept.", "year": 2018}}, {"citedPaper": {"paperId": "43962ec59c5ef78271381842285c7d8dfc3f0d57", "url": "https://www.semanticscholar.org/paper/43962ec59c5ef78271381842285c7d8dfc3f0d57", "title": "An Adversarial Approach for Explainable AI in Intrusion Detection Systems", "abstract": "Despite the growing popularity of modern machine learning techniques (e.g, Deep Neural Networks) in cyber-security applications, most of these models are perceived as a black-box for the user. Adversarial machine learning offers an approach to increase our understanding of these models. In this paper we present an approach to generate explanations for incorrect classifications made by data-driven Intrusion Detection Systems (IDSs) An adversarial approach is used to find the minimum modifications (of the input features) required to correctly classify a given set of misclassified samples. The magnitude of such modifications is used to visualize the most relevant features that explain the reason for the misclassification. The presented methodology generated satisfactory explanations that describe the reasoning behind the mis-classifications, with descriptions that match expert knowledge. The advantages of the presented methodology are: 1) applicable to any classifier with defined gradients. 2) does not require any modification of the classifier model. 3) can be extended to perform further diagnosis (e.g. vulnerability assessment) and gain further understanding of the system. Experimental evaluation was conducted on the NSL-KDD99 benchmark dataset using Linear and Multilayer perceptron classifiers. The results are shown using intuitive visualizations in order to improve the interpretability of the results.", "year": 2018}}, {"citedPaper": {"paperId": "21dff47a4142445f83016da0819ffe6dd2947f66", "url": "https://www.semanticscholar.org/paper/21dff47a4142445f83016da0819ffe6dd2947f66", "title": "Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)", "abstract": "At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.", "year": 2018}}, {"citedPaper": {"paperId": "393d8052ed57d71de2bd4a3c05bc5e42e4072b02", "url": "https://www.semanticscholar.org/paper/393d8052ed57d71de2bd4a3c05bc5e42e4072b02", "title": "SCADA System Testbed for Cybersecurity Research Using Machine Learning Approach", "abstract": "This paper presents the development of a Supervisory Control and Data Acquisition (SCADA) system testbed used for cybersecurity research. The testbed consists of a water storage tank\u2019s control system, which is a stage in the process of water treatment and distribution. Sophisticated cyber-attacks were conducted against the testbed. During the attacks, the network traffic was captured, and features were extracted from the traffic to build a dataset for training and testing different machine learning algorithms. Five traditional machine learning algorithms were trained to detect the attacks: Random Forest, Decision Tree, Logistic Regression, Na\u00efve Bayes and KNN. Then, the trained machine learning models were built and deployed in the network, where new tests were made using online network traffic. The performance obtained during the training and testing of the machine learning models was compared to the performance obtained during the online deployment of these models in the network. The results show the efficiency of the machine learning models in detecting the attacks in real time. The testbed provides a good understanding of the effects and consequences of attacks on real SCADA environments.", "year": 2018}}, {"citedPaper": {"paperId": "79c26cce7bffcfc5c1e9fbafe04e921d67bd52e5", "url": "https://www.semanticscholar.org/paper/79c26cce7bffcfc5c1e9fbafe04e921d67bd52e5", "title": "Toward Explainable Deep Neural Network Based Anomaly Detection", "abstract": "Anomaly detection in industrial processes is crucial for general process monitoring and process health assessment. Deep Neural Networks (DNNs) based anomaly detection has received increased attention in recent work. Albeit their high accuracy, the black-box nature of DNNs is a drawback in practical deployment. Especially in industrial anomaly detection systems, explanations of DNN detected anomalies are crucial. This paper presents a framework for DNN based anomaly detection which provides explanations of detected anomalies. The framework answers the following questions during online processing: 1) \u201cwhy is it an anomaly?\u201d and 2) \u201cwhat is the confidence?\u201d Further, the framework can be used offline to evaluate the \u201cknowledge\u201d of the trained DNN. The framework reduces the opaqueness of the DNN based anomaly detector and thus improves human operators' trust in the algorithm. This paper implements the first steps of the presented framework on the benchmark KDD-NSL dataset for Denial of Service (DoS) attack detection. Offline DNN explanations showed that the DNN was detecting DoS attacks based on features indicating destination of connection, frequency and amount of data transferred while showing an accuracy around 97%.", "year": 2018}}, {"citedPaper": {"paperId": "7d065e649e3bfc7d6d36166f50eab37b8404eae0", "url": "https://www.semanticscholar.org/paper/7d065e649e3bfc7d6d36166f50eab37b8404eae0", "title": "Interpretable Machine Learning in Healthcare", "abstract": "This tutorial extensively covers the definitions, nuances, challenges, and requirements for the design of interpretable and explainable machine learning models and systems in healthcare. We discuss many uses in which interpretable machine learning models are needed in healthcare and how they should be deployed. Additionally, we explore the landscape of recent advances to address the challenges model interpretability in healthcare and also describe how one would go about choosing the right interpretable machine learnig algorithm for a given problem in healthcare.", "year": 2018}}, {"citedPaper": {"paperId": "1d8f4f76ac6534627ef8a1c24b9937d8ab2a5c5f", "url": "https://www.semanticscholar.org/paper/1d8f4f76ac6534627ef8a1c24b9937d8ab2a5c5f", "title": "Anchors: High-Precision Model-Agnostic Explanations", "abstract": "\n \n We introduce a novel model-agnostic system that explains the behavior of complex models with high-precision rules called anchors, representing local, \"sufficient\" conditions for predictions. We propose an algorithm to efficiently compute these explanations for any black-box model with high-probability guarantees. We demonstrate the flexibility of anchors by explaining a myriad of different models for different domains and tasks. In a user study, we show that anchors enable users to predict how a model would behave on unseen instances with less effort and higher precision, as compared to existing linear explanations or no explanations.\n \n", "year": 2018}}, {"citedPaper": {"paperId": "f7325d232c7ac7d2daaf6605377058db5b5b83cc", "url": "https://www.semanticscholar.org/paper/f7325d232c7ac7d2daaf6605377058db5b5b83cc", "title": "A Survey of Methods for Explaining Black Box Models", "abstract": "In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, it explicitly or implicitly delineates its own definition of interpretability and explanation. The aim of this article is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation, this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.", "year": 2018}}, {"citedPaper": {"paperId": "c194307b9dda7f53a0c639319d2ec1c23b86e6a6", "url": "https://www.semanticscholar.org/paper/c194307b9dda7f53a0c639319d2ec1c23b86e6a6", "title": "What do we need to build explainable AI systems for the medical domain?", "abstract": "Artificial intelligence (AI) generally and machine learning (ML) specifically demonstrate impressive practical success in many different application domains, e.g. in autonomous driving, speech recognition, or recommender systems. Deep learning approaches, trained on extremely large data sets or using reinforcement learning methods have even exceeded human performance in visual tasks, particularly on playing games such as Atari, or mastering the game of Go. Even in the medical domain there are remarkable results. The central problem of such models is that they are regarded as black-box models and even if we understand the underlying mathematical principles, they lack an explicit declarative knowledge representation, hence have difficulty in generating the underlying explanatory structures. This calls for systems enabling to make decisions transparent, understandable and explainable. A huge motivation for our approach are rising legal and privacy aspects. The new European General Data Protection Regulation entering into force on May 25th 2018, will make black-box approaches difficult to use in business. This does not imply a ban on automatic learning approaches or an obligation to explain everything all the time, however, there must be a possibility to make the results re-traceable on demand. In this paper we outline some of our research topics in the context of the relatively new area of explainable-AI with a focus on the application in medicine, which is a very special domain. This is due to the fact that medical professionals are working mostly with distributed heterogeneous and complex sources of data. In this paper we concentrate on three sources: images, *omics data and text. We argue that research in explainable-AI would generally help to facilitate the implementation of AI/ML in the medical domain, and specifically help to facilitate transparency and trust.", "year": 2017}}, {"citedPaper": {"paperId": "e89dfa306723e8ef031765e9c44e5f6f94fd8fda", "url": "https://www.semanticscholar.org/paper/e89dfa306723e8ef031765e9c44e5f6f94fd8fda", "title": "Explanation in Artificial Intelligence: Insights from the Social Sciences", "abstract": null, "year": 2017}}, {"citedPaper": {"paperId": "442e10a3c6640ded9408622005e3c2a8906ce4c2", "url": "https://www.semanticscholar.org/paper/442e10a3c6640ded9408622005e3c2a8906ce4c2", "title": "A Unified Approach to Interpreting Model Predictions", "abstract": "Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.", "year": 2017}}, {"citedPaper": {"paperId": "7f01afccffe4eb5f1392aa6639cd29b672376840", "url": "https://www.semanticscholar.org/paper/7f01afccffe4eb5f1392aa6639cd29b672376840", "title": "Industry 4.0: The Industrial Internet of Things", "abstract": "Explore the current state of the production, processing, and manufacturing industries and discover what it will take to achieve re-industrialization of the former industrial powerhouses that can counterbalance the benefits of cheap labor providers dominating the industrial sector. This book explores the potential for the Internet of Things (IoT), Big Data, Cyber-Physical Systems (CPS), and Smart Factory technologies to replace the still largely mechanical, people-based systems of offshore locations.Industry 4.0: The Industrial Internet of Things covers Industry 4.0, a term that encapsulates trends and technologies that could rewrite the rules of manufacturing and production.What You'll Learn:What are the Industrial Internet and Industrial Internet of ThingsWhich technologies must advance to enable Industry 4.0What is happening today to make that happenWhat are examples of the implementation of Industry 4.0How to apply some of these case studiesWhat is the potential to take back the lead in manufacturing, and the potential fallout that could resultWho This Book is For: Business futurists, business strategists, CEOs and CTOs, and anyone with an interest and an IT or business background; or anyone who may have a keen interest in how the future of IT, industry and production will develop over the next two decades.", "year": 2016}}, {"citedPaper": {"paperId": "c0883f5930a232a9c1ad601c978caede29155979", "url": "https://www.semanticscholar.org/paper/c0883f5930a232a9c1ad601c978caede29155979", "title": "\u201cWhy Should I Trust You?\u201d: Explaining the Predictions of Any Classifier", "abstract": "Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.", "year": 2016}}, {"citedPaper": {"paperId": "0e7af8e91b8cb2cea1164be5ac5d280b0d12c153", "url": "https://www.semanticscholar.org/paper/0e7af8e91b8cb2cea1164be5ac5d280b0d12c153", "title": "UNSW-NB15: a comprehensive data set for network intrusion detection systems (UNSW-NB15 network data set)", "abstract": "One of the major research challenges in this field is the unavailability of a comprehensive network based data set which can reflect modern network traffic scenarios, vast varieties of low footprint intrusions and depth structured information about the network traffic. Evaluating network intrusion detection systems research efforts, KDD98, KDDCUP99 and NSLKDD benchmark data sets were generated a decade ago. However, numerous current studies showed that for the current network threat environment, these data sets do not inclusively reflect network traffic and modern low footprint attacks. Countering the unavailability of network benchmark data set challenges, this paper examines a UNSW-NB15 data set creation. This data set has a hybrid of the real modern normal and the contemporary synthesized attack activities of the network traffic. Existing and novel methods are utilised to generate the features of the UNSWNB15 data set. This data set is available for research purposes and can be accessed from the link.", "year": 2015}}, {"citedPaper": {"paperId": "de5537c5b0f9bca507420d51b25eb02a0d1cd021", "url": "https://www.semanticscholar.org/paper/de5537c5b0f9bca507420d51b25eb02a0d1cd021", "title": "Multiple Factor Analysis by Example Using R", "abstract": "Principal Component Analysis Multiple Correspondence Analysis Factor Analysis for Mixed Data Weighting Groups of Variables Comparing Clouds of Partial Individuals Factors Common to Different Groups of Variables Comparing Groups of Variables and Indscal Model Qualitative and Mixed Data Multiple Factor Analysis and Procrustes Analysis Hierarchical Multiple Factor Analysis Matrix Calculus and Euclidean Vector Space Bibliography", "year": 2014}}, {"citedPaper": {"paperId": "168f28ac3c8c7ea63bf7ed25f2288e8b67e2fe74", "url": "https://www.semanticscholar.org/paper/168f28ac3c8c7ea63bf7ed25f2288e8b67e2fe74", "title": "Scikit-learn: Machine Learning in Python", "abstract": "Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.", "year": 2011}}, {"citedPaper": {"paperId": "fc3eb090e39d71295c362458b8a0c48d2c5d8377", "url": "https://www.semanticscholar.org/paper/fc3eb090e39d71295c362458b8a0c48d2c5d8377", "title": "A detailed analysis of the KDD CUP 99 data set", "abstract": "During the last decade, anomaly detection has attracted the attention of many researchers to overcome the weakness of signature-based IDSs in detecting novel attacks, and KDDCUP'99 is the mostly widely used data set for the evaluation of these systems. Having conducted a statistical analysis on this data set, we found two important issues which highly affects the performance of evaluated systems, and results in a very poor evaluation of anomaly detection approaches. To solve these issues, we have proposed a new data set, NSL-KDD, which consists of selected records of the complete KDD data set and does not suffer from any of mentioned shortcomings.", "year": 2009}}, {"citedPaper": {"paperId": "d6283a32c48c8989c039127a187198a1325c7024", "url": "https://www.semanticscholar.org/paper/d6283a32c48c8989c039127a187198a1325c7024", "title": "Designing Experiments and Analyzing Data: a Model Comparison Perspective, 2nd edn", "abstract": null, "year": 2005}}, {"citedPaper": {"paperId": "eabd1b592e9f5b562052ed6046a292cac386bf6c", "url": "https://www.semanticscholar.org/paper/eabd1b592e9f5b562052ed6046a292cac386bf6c", "title": "The Art of Computer Systems Performance Analysis: Techniques for Experimental Design, Measurement, Simulation, and Modeling (Raj Jain)", "abstract": "Description: The Art of Computer Systems Performance Analysis \"At last, a welcome and needed text for computer professionals who require practical, ready-to-apply techniques for performance analysis. Highly recommended!\" -Dr. Leonard Kleinrock University of California, Los Angeles \"An entirely refreshing text which has just the right mixture of theory and real world practice. The book is ideal for both classroom instruction and self-study.\" -Dr. Raymond L. Pickholtz President, IEEE Communications Society \"An extraordinarily comprehensive treatment of both theoretical and practical issues.\" -Dr. Jeffrey P. Buzen Internationally recognized performance analysis expert \". it is the most thorough book available to date\" -Dr. Erol Gelenbe Universit? Ren? Descartes, Paris \". an extraordinary book.. A worthy addition to the bookshelf of any practicing computer or communications engineer\" -Dr. Vinton G. Cer??? Chairman, ACM SIGCOMM \"This is an unusual object, a textbook that one wants to sit down and peruse. The prose is clear and fluent, but more important, it is witty.\" -Allison Mankin The Mitre Washington Networking Center Newsletter", "year": 1992}}, {"citedPaper": {"paperId": null, "url": null, "title": "Interpretable machine learning. A guide for making black box models explainable, 2018, available at: https://christophm.github.io/ interpretable-ml-book", "abstract": null, "year": 2022}}, {"citedPaper": {"paperId": "641ea570259679b9913d1cacadd8356ed1398149", "url": "https://www.semanticscholar.org/paper/641ea570259679b9913d1cacadd8356ed1398149", "title": "Explainable Artificial Intelligence (xAI) Approaches and Deep Meta-Learning Models for Cyber-Physical Systems", "abstract": "Today, the effects of promising technologies such as explainable artificial intelligence (xAI) and meta-learning (ML) on the internet of things (IoT) and the cyber-physical systems (CPS), which are important components of Industry 4.0, are increasingly intensified. However, there are important shortcomings that current deep learning models are currently inadequate. These artificial neural network based models are black box models that generalize the data transmitted to it and learn from the data. Therefore, the relational link between input and output is not observable. For these reasons, it is necessary to make serious efforts on the explanability and interpretability of black box models. In the near future, the integration of explainable artificial intelligence and meta-learning approaches to cyber-physical systems will have effects on a high level of virtualization and simulation infrastructure, real-time supply chain, cyber factories with smart machines communicating over the internet, maximizing production efficiency, analysis of service quality and competition level.", "year": 2021}}, {"citedPaper": {"paperId": "c785e1453be0cc374bb84fb589123b697dfc3497", "url": "https://www.semanticscholar.org/paper/c785e1453be0cc374bb84fb589123b697dfc3497", "title": "An Explainable Machine Learning Framework for Intrusion Detection Systems", "abstract": "In recent years, machine learning-based intrusion detection systems (IDSs) have proven to be effective; especially, deep neural networks improve the detection rates of intrusion detection models. However, as models become more and more complex, people can hardly get the explanations behind their decisions. At the same time, most of the works about model interpretation focuses on other fields like computer vision, natural language processing, and biology. This leads to the fact that in practical use, cybersecurity experts can hardly optimize their decisions according to the judgments of the model. To solve these issues, a framework is proposed in this paper to give an explanation for IDSs. This framework uses SHapley Additive exPlanations (SHAP), and combines local and global explanations to improve the interpretation of IDSs. The local explanations give the reasons why the model makes certain decisions on the specific input. The global explanations give the important features extracted from IDSs, present the relationships between the feature values and different types of attacks. At the same time, the interpretations between two different classifiers, one-vs-all classifier and multiclass classifier, are compared. NSL-KDD dataset is used to test the feasibility of the framework. The framework proposed in this paper leads to improve the transparency of any IDS, and helps the cybersecurity staff have a better understanding of IDSs\u2019 judgments. Furthermore, the different interpretations between different kinds of classifiers can also help security experts better design the structures of the IDSs. More importantly, this work is unique in the intrusion detection field, presenting the first use of the SHAP method to give explanations for IDSs.", "year": 2020}}, {"citedPaper": {"paperId": "91b8e826be77c1f292f2fbf365e2203255bdc654", "url": "https://www.semanticscholar.org/paper/91b8e826be77c1f292f2fbf365e2203255bdc654", "title": "Explainable Artificial Intelligence and its potential within Industry", "abstract": "The age of Big Data has enabled the creation of arti\ufb01cial intelligence solutions that has allowed systems to better respond to their users requests and needs. Applications such as recommender systems, automated content generation systems, etc. are increasingly leveraging such large amounts of data to make better informed decisions about how to tailor their output appropriately. However, the opaqueness of these AI systems in how they derive their decisions or outputs has led to an increasing call for transparency with increasing concerns for the potential of bias to occur in areas such as \ufb01nance and criminal law. The culmination of these calls have lead to tentative legislative steps. For example, the \"Right to explanation\" as part of the recently enacted European Union\u2019s General Data Protection Regulation. Natural Language Generation (NLG) has been used in successfully in many data-to-text applications allowing users to gain insights from their data sets. Whilst NLG technology has a strong role to play in generating explanations for AI models there still remains inherit challenges in developing and deploying text generation systems within a commercial context. In this talk I will explore the role and potential that Natural Language Explainable AI can have within trivago and the wider industry. trivago is a leading accommodation meta-search engine that enables users \ufb01nd the right hotel or apartment at the right price. In particular, this talk will describe the work we have done to apply natural language solutions within trivago and the challenges of applying AI solutions from a commercial perspective. Finally, this talk will also explore the potential applications of where explainable AI approaches could be used within trivago .", "year": 2019}}, {"citedPaper": {"paperId": null, "url": null, "title": "Security and Privacy Trends in The Industrial Internet of Things, 1st ed. Cham, Switzerland", "abstract": null, "year": 2019}}, {"citedPaper": {"paperId": "0e9d134411fd68ba2d2d5c48cdafdb0650088ebe", "url": "https://www.semanticscholar.org/paper/0e9d134411fd68ba2d2d5c48cdafdb0650088ebe", "title": "Explainable AI Driving business value through greater understanding", "abstract": null, "year": 2018}}, {"citedPaper": {"paperId": null, "url": null, "title": "Gartner says nearly half of CIOs are planning to deploy arti\ufb01cial intelligence", "abstract": null, "year": 2018}}, {"citedPaper": {"paperId": null, "url": null, "title": "Feature selection: A data perspective", "abstract": null, "year": 2017}}, {"citedPaper": {"paperId": null, "url": null, "title": "Practical Guide To Principal Component Methods in R", "abstract": null, "year": 2017}}, {"citedPaper": {"paperId": null, "url": null, "title": "Game changers: AI and machine learning in cyberse-curity", "abstract": null, "year": 2017}}, {"citedPaper": {"paperId": "5387c0769d3d18ace289c5cb3dcf90231d9d425f", "url": "https://www.semanticscholar.org/paper/5387c0769d3d18ace289c5cb3dcf90231d9d425f", "title": "Elements of information theory (2. ed.)", "abstract": null, "year": 2006}}, {"citedPaper": {"paperId": null, "url": null, "title": ", and G . J . McLachlan , \u201c The EM algorithm , \u201d in Handbook of computational statistics", "abstract": null, "year": 2006}}, {"citedPaper": {"paperId": "7fe53db84b1cfb1c2b4abab4c90308c764c42245", "url": "https://www.semanticscholar.org/paper/7fe53db84b1cfb1c2b4abab4c90308c764c42245", "title": "Designing Experiments and Analyzing Data: A Model Comparison Perspective", "abstract": "Contents: Preface. Part I: Conceptual Bases of Experimental Design and Analysis. The Logic of Experimental Design. Introduction to the Fisher Tradition. Part II: Model Comparisons for Between-Subjects Designs. Introduction to Model Comparisons: One-Way Between-Subjects Designs. Individual Comparisons of Means. Testing Several Contrasts: The Multiple-Comparison Problem. Trend Analysis. Two-Way Between-Subjects Factorial Designs. Higher Order Between-Subjects Factorial Designs. Designs With Covariates: ANCOVA and Blocking. Designs With Random or Nested Factors. Part III: Model Comparisons for Designs Involving Within-Subjects Factors. One-Way Within-Subjects Designs: Univariate Approach. Higher-Order Designs With Within-Subjects Factors: Univariate Approach. One-Way Within-Subjects Designs: Multivariate Approach. Higher Order Designs With Within-Subjects Factors: Multivariate Approach. Part IV: Alternative Analysis Strategies. An Introduction to Multilevel Models for Within-Subjects Designs. An Introduction to Multilevel Hierarchical Mixed Models: Nested Designs. Appendices: Statistical Tables. Part 1. Linear Models: The Relation Between ANOVA and Regression. Part 2. A Brief Primer of Principles of Formulating and Comparing Models. Notes. Solutions to Selected Exercises. References.", "year": 1990}}, {"citedPaper": {"paperId": "313773dc6472a443d4ecc8fb15b5b1640ae6b49c", "url": "https://www.semanticscholar.org/paper/313773dc6472a443d4ecc8fb15b5b1640ae6b49c", "title": "Der Open-access-publikationsserver Der Zbw \u2013 Leibniz-informationszentrum Wirtschaft the Open Access Publication Server of the Zbw \u2013 Leibniz Information Centre for Economics Does Macroeconomics Need Microeconomic Foundations? Does Macroeconomics Need Microeconomic Foundations? Licensed under a Creati", "abstract": "Terms of use: The ZBW grants you, the user, the non-exclusive right to use the selected work free of charge, territorially unrestricted and within the time limit of the term of the property rights according to the terms specified at \u2192 http://www.econstor.eu/dspace/Nutzungsbedingungen By the first use of the selected work the user agrees and declares to comply with these terms of use. Abstract The author argues that it is microeconomics that needs foundations, not macroeconomics. Preferences need to be built on biology, and, in particular, on neuroscience. In contrast, macroeconomics could benefit from rationalizations of aggregate economic phenomena by non-equilibrium statistical physics. Paper submitted to the special issue \" Reconstructing Macroeconomics \"", "year": null}}, {"citedPaper": {"paperId": null, "url": null, "title": "We show that TRUST XAI is very fast at reasoning the primary AI model\u2019s behavior on numerical data, making it one of the \ufb01rst XAI models suitable for real-time numerical applications", "abstract": null, "year": null}}, {"citedPaper": {"paperId": null, "url": null, "title": "Our case study introduces a unique perspective on the assessment of AI-based trustworthy systems for IIoT and Industry 4.0", "abstract": null, "year": null}}]